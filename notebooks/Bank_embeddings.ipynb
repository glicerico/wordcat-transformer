{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with BERT first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings from target word in a sentence\n",
    "- Can be a target that gets sub-worded, and appear any number of times in the sentence (inc. zero)\n",
    "- Using last layer hidden state as embedding\n",
    "- Using first sub-token in target as embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = \"banka\"\n",
    "test_sent = \"They stole all the money in the bank.\"\n",
    "#target_word = \"puppeteer\"\n",
    "#test_sent = \"The puppeteer show and the puppeteer came late.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'they', 'stole', 'all', 'the', 'money', 'in', 'the', 'bank', '.', '[SEP]']\n",
      "tensor([[  101,  2027, 10312,  2035,  1996,  2769,  1999,  1996,  2924,  1012,\n",
      "           102]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(test_sent, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "print(tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "print(input_ids)\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "target_ids = tokenizer.encode(target_word, add_special_tokens=False)\n",
    "# USE REPRESENTATION OF FIRST SUB-TOKEN AS EMBEDDING (Following Devlin et al 2019)\n",
    "inputs_list = input_ids[0].tolist()\n",
    "target_len = len(target_ids)\n",
    "matches = [target_ids == inputs_list[i:i+target_len] for i in range(len(inputs_list) - target_len + 1)]\n",
    "matches_id = [j for j, val in enumerate(matches) if val]\n",
    "print(matches_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# It's possible to have the same word in the sentence twice\n",
    "for match in matches_id:\n",
    "    print(outputs[0][0][match])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training corpus to visualize embeddings\n",
    "Data from the UFSAC repository corpora, version 2.1. Same as used by Wiedemann et al. (2019).\n",
    "\n",
    "Repository: https://github.com/getalp/UFSAC\n",
    "\n",
    "Data link: https://drive.google.com/file/d/1Oigo3kzRosz2VjyA44vpJZ58tDFyLRMO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers]",
   "language": "python",
   "name": "conda-env-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
