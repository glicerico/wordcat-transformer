{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word category formation using BERT predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea is to generate word embeddings by:\n",
    "- Get a list of sentences\n",
    "- Mask a word in each sentence (repeat a sentence in the list if you want to mask different positions)\n",
    "- For each sentence, obtain the logit vector for the masked word from BERT's prediction (last hidden layer)\n",
    "- Cluster sentences logit vectors. The clusters should reflect words that fit together both syntactically and semantically.\n",
    "- Build each word category by finding the highest valued words in the vectors belonging to a cluster (perhaps by most common top words, all words above some threshold, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import numpy as np\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebca8599b294ab0a9fe1e2ca2d6c7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=361.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose some simple sentences with masked adjectives and nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sentences = \"\"\"_ fat cat ate the mouse.\n",
    "The _ cat ate the mouse.\n",
    "The fat _ ate the mouse.\n",
    "The fat cat _ the mouse.\n",
    "The fat cat ate _ mouse.\n",
    "The fat cat ate the _.\n",
    "_ was wearing a lovely satin dress last night.\n",
    "She _ wearing a lovely satin dress last night.\n",
    "She was _ a lovely satin dress last night.\n",
    "She was wearing _ lovely satin dress last night.\n",
    "She was wearing a _ satin dress last night.\n",
    "She was wearing a lovely _ dress last night.\n",
    "She was wearing a lovely satin _ last night.\n",
    "She was wearing a lovely satin dress _ night.\n",
    "She was wearing a lovely satin dress last _.\n",
    "_ was receiving quite a hefty salary.\n",
    "He _ receiving quite a hefty salary.\n",
    "He was _ quite a hefty salary.\n",
    "He was receiving _ a hefty salary.\n",
    "He was receiving quite _ hefty salary.\n",
    "He was receiving quite a _ salary.\n",
    "He was receiving quite a hefty _.\n",
    "_ also bought a used sofa for his new apartment.\n",
    "He _ bought a used sofa for his new apartment.\n",
    "He also _ a used sofa for his new apartment.\n",
    "He also bought _ used sofa for his new apartment.\n",
    "He also bought a _ sofa for his new apartment.\n",
    "He also bought a used _ for his new apartment.\n",
    "He also bought a used sofa _ his new apartment.\n",
    "He also bought a used sofa for _ new apartment.\n",
    "He also bought a used sofa for his _ apartment.\n",
    "He also bought a used sofa for his new _.\n",
    "_ was born and grew up in Havana.\n",
    "I _ born and grew up in Havana.\n",
    "I was _ and grew up in Havana.\n",
    "I was born _ grew up in Havana.\n",
    "I was born and _ up in Havana.\n",
    "I was born and grew _ in Havana.\n",
    "I was born and grew up _ Havana.\n",
    "I was born and grew up in _.\n",
    "_ Beijing metropolitan area added more than a million people in the past decade.\n",
    "The _ metropolitan area added more than a million people in the past decade.\n",
    "The Beijing _ area added more than a million people in the past decade.\n",
    "The Beijing metropolitan _ added more than a million people in the past decade.\n",
    "The Beijing metropolitan area _ more than a million people in the past decade.\n",
    "The Beijing metropolitan area added _ than a million people in the past decade.\n",
    "The Beijing metropolitan area added more _ a million people in the past decade.\n",
    "The Beijing metropolitan area added more than _ million people in the past decade.\n",
    "The Beijing metropolitan area added more than a _ people in the past decade.\n",
    "The Beijing metropolitan area added more than a million _ in the past decade.\n",
    "The Beijing metropolitan area added more than a million people _ the past decade.\n",
    "The Beijing metropolitan area added more than a million people in _ past decade.\n",
    "The Beijing metropolitan area added more than a million people in the _ decade.\n",
    "The Beijing metropolitan area added more than a million people in the past _.\n",
    "_ races are held around the lake and farmlands.\n",
    "Bike _ are held around the lake and farmlands.\n",
    "Bike races _ held around the lake and farmlands.\n",
    "Bike races are _ around the lake and farmlands.\n",
    "Bike races are held _ the lake and farmlands.\n",
    "Bike races are held around _ lake and farmlands.\n",
    "Bike races are held around the _ and farmlands.\n",
    "Bike races are held around the lake _ farmlands.\n",
    "Bike races are held around the lake and _.\n",
    "_ racist cousin called me last night.\n",
    "My _ cousin called me last night.\n",
    "My racist _ called me last night.\n",
    "My racist cousin _ me last night.\n",
    "My racist cousin called _ last night.\n",
    "My racist cousin called me _ night.\n",
    "My racist cousin called me last _.\n",
    "_ device is considered to be available if it is not being used by another adult.\n",
    "A _ is considered to be available if it is not being used by another adult.\n",
    "A device _ considered to be available if it is not being used by another adult.\n",
    "A device is _ to be available if it is not being used by another adult.\n",
    "A device is considered _ be available if it is not being used by another adult.\n",
    "A device is considered to _ available if it is not being used by another adult.\n",
    "A device is considered to be _ if it is not being used by another adult.\n",
    "A device is considered to be available _ it is not being used by another adult.\n",
    "A device is considered to be available if _ is not being used by another adult.\n",
    "A device is considered to be available if it _ not being used by another adult.\n",
    "A device is considered to be available if it is _ being used by another adult.\n",
    "A device is considered to be available if it is not _ used by another adult.\n",
    "A device is considered to be available if it is not being _ by another adult.\n",
    "A device is considered to be available if it is not being used _ another adult.\n",
    "A device is considered to be available if it is not being used by _ adult.\n",
    "A device is considered to be available if it is not being used by another _.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sentences = \"\"\"The _ cat ate the mouse.\n",
    "She was wearing a lovely _ dress last night.\n",
    "He was receiving quite a _ salary.\n",
    "He also bought a _ sofa for his new apartment.\n",
    "I was born and grew up in _.\n",
    "The _ metropolitan area added more than a million people in the past decade.\n",
    "Bike races are held around the _ and farmlands.\n",
    "My racist _ called me last night.\n",
    "A device is considered to be available if it is not being used by another _.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process sentences with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place [MASK] tokens\n",
    "MASK = '[MASK]'\n",
    "sentences = re.sub(r'\\b_+\\b', '[MASK]', text_sentences).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tokenize input\n",
    "input_ids = [tokenizer.encode(s, add_special_tokens=True) for s in sentences]\n",
    "\n",
    "# Find location of MASKS\n",
    "tok_MASK = tokenizer.convert_tokens_to_ids(MASK)\n",
    "mask_positions = [s.index(tok_MASK) for s in input_ids] \n",
    "\n",
    "# Make all sentence arrays equal length by padding\n",
    "max_len = max(len(i) for i in input_ids)\n",
    "padded_input = np.array([i + [0]* (max_len - len(i)) for i in input_ids])\n",
    "\n",
    "attention_mask = np.where(padded_input != 0, 1, 0)  # Create mask to ignore padding\n",
    "\n",
    "input = torch.tensor(padded_input)\n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hidden layers\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for the masked word of each sentence\n",
    "embeddings = [lh[m].numpy() for lh, m in zip(last_hidden_states[0], mask_positions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_predictions(probs, k=5, thres=0.01):\n",
    "    \"\"\"\n",
    "    Print and return top-k predictions for a given probs list.\n",
    "    Also return predictions above threshold\n",
    "    \"\"\"\n",
    "    # Get top-k tokens\n",
    "    probs = probs.detach().numpy()\n",
    "    top_indexes = np.argpartition(probs, -k)[-k:]\n",
    "    sorted_indexes = top_indexes[np.argsort(-probs[top_indexes])]\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(sorted_indexes)\n",
    "    print(f\"Ordered top predicted tokens: {top_tokens}\")\n",
    "    print(f\"Ordered top predicted values: {probs[sorted_indexes]}\\n\")\n",
    "    \n",
    "    # Get all tokens above threshold\n",
    "    high_indexes = np.where(probs > thres)\n",
    "    high_tokens = tokenizer.convert_ids_to_tokens(high_indexes[0])\n",
    "    return top_tokens, high_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert last layer logit predictions to probabilities\n",
    "We can see what are the highest predictions for the blank in each sentence, and their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\n",
      "The [MASK] cat ate the mouse.\n",
      "Ordered top predicted tokens: ['black', 'cheshire', 'big', 'little', 'fat']\n",
      "Ordered top predicted values: [0.13267049 0.08640933 0.06516975 0.03538685 0.03100599]\n",
      "\n",
      "Sentence:\n",
      "She was wearing a lovely [MASK] dress last night.\n",
      "Ordered top predicted tokens: ['white', 'black', 'red', 'pink', 'blue']\n",
      "Ordered top predicted values: [0.20945124 0.16496556 0.13129269 0.08869011 0.05542691]\n",
      "\n",
      "Sentence:\n",
      "He was receiving quite a [MASK] salary.\n",
      "Ordered top predicted tokens: ['good', 'handsome', 'high', 'generous', 'decent']\n",
      "Ordered top predicted values: [0.18829058 0.09613485 0.09576207 0.0917473  0.0544567 ]\n",
      "\n",
      "Sentence:\n",
      "He also bought a [MASK] sofa for his new apartment.\n",
      "Ordered top predicted tokens: ['new', 'comfortable', 'luxurious', 'large', 'luxury']\n",
      "Ordered top predicted values: [0.6247967  0.05839209 0.02877485 0.0248212  0.01501671]\n",
      "\n",
      "Sentence:\n",
      "I was born and grew up in [MASK].\n",
      "Ordered top predicted tokens: ['chicago', 'california', 'texas', 'london', 'england']\n",
      "Ordered top predicted values: [0.03689728 0.03364525 0.02513845 0.02230389 0.018717  ]\n",
      "\n",
      "Sentence:\n",
      "The [MASK] metropolitan area added more than a million people in the past decade.\n",
      "Ordered top predicted tokens: ['chicago', 'washington', 'seattle', 'atlanta', 'detroit']\n",
      "Ordered top predicted values: [0.22472836 0.05055556 0.03419264 0.03207787 0.02761591]\n",
      "\n",
      "Sentence:\n",
      "Bike races are held around the [MASK] and farmlands.\n",
      "Ordered top predicted tokens: ['village', 'park', 'town', 'city', 'forest']\n",
      "Ordered top predicted values: [0.15070313 0.09536388 0.0665911  0.05703236 0.04966085]\n",
      "\n",
      "Sentence:\n",
      "My racist [MASK] called me last night.\n",
      "Ordered top predicted tokens: ['friend', 'neighbor', 'boyfriend', 'father', 'brother']\n",
      "Ordered top predicted values: [0.24061793 0.0719889  0.06706642 0.06234702 0.05771114]\n",
      "\n",
      "Sentence:\n",
      "A device is considered to be available if it is not being used by another [MASK].\n",
      "Ordered top predicted tokens: ['person', 'user', 'party', 'device', 'entity']\n",
      "Ordered top predicted values: [0.27529848 0.25013295 0.09434847 0.07847174 0.04001943]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert last hidden state to probs and find tokens\n",
    "sm = torch.nn.Softmax(dim=0) \n",
    "#id_large = tokenizer.convert_tokens_to_ids('large')\n",
    "all_high_tokens = []\n",
    "i = 0\n",
    "for lh, m in zip(last_hidden_states[0], mask_positions):\n",
    "    print(\"Sentence:\")\n",
    "    print(sentences[i])\n",
    "    i += 1\n",
    "    probs = sm(lh[m])\n",
    "    #print(f\"Probability of 'large': {probs[id_large]}\")\n",
    "    _, high_tokens = get_top_predictions(probs)\n",
    "    all_high_tokens.append(high_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 3, 3, 4, 2, 5], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cluster embeddings with KMeans\n",
    "from sklearn.cluster import KMeans, OPTICS, DBSCAN, cluster_optics_dbscan\n",
    "k = 6\n",
    "estimator = KMeans(init=\"k-means++\", n_clusters=k, n_jobs=4)\n",
    "estimator.fit(embeddings)\n",
    "estimator.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form word categories\n",
    "Take all words above a threshold from vectors that belong to a cluster to form word categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0:\n",
      "nice, modest, handsome, decent, high, comfortable, steady, substantial, generous, respectable, low, small, fine, large, considerable, good\n",
      "\n",
      "Category 1:\n",
      "little, evening, new, black, luxury, great, silver, blue, white, gray, yellow, old, green, brown, comfortable, small, giant, leather, silk, pink, red, dead, mother, big, wild, luxurious, purple, wedding, large, fat, cheshire\n",
      "\n",
      "Category 2:\n",
      "father, cousin, friends, boss, neighbors, husband, friend, dad, uncle, neighbor, wife, boyfriend, partner, mother, girlfriend, brother, roommate\n",
      "\n",
      "Category 3:\n",
      "indianapolis, washington, toronto, london, atlanta, california, austin, cleveland, mexico, brooklyn, philadelphia, denver, chicago, minneapolis, florida, seattle, portland, dallas, germany, detroit, france, louisville, texas, england, pittsburgh, houston, canada\n",
      "\n",
      "Category 4:\n",
      "village, forest, hills, forests, gardens, countryside, towns, parks, lake, park, lakes, town, mountains, city, farms, villages, fields, woods\n",
      "\n",
      "Category 5:\n",
      "user, party, application, device, entity, company, person, customer, provider, organization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_categories = {}\n",
    "for cl in range(k):\n",
    "    cluster_members = np.where(estimator.labels_ == cl)\n",
    "    word_categories[cl] = sum((all_high_tokens[i] for i in cluster_members[0]), [])\n",
    "    word_categories[cl] = set(word_categories[cl])\n",
    "    print(f\"Category {cl}:\")\n",
    "    print(\", \".join(word_categories[cl]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers]",
   "language": "python",
   "name": "conda-env-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
