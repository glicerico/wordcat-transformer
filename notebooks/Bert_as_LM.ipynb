{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate sentence probability with BERT\n",
    "## From https://github.com/huggingface/transformers/issues/37, with bugs fixed and updated to newest transformers version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "with torch.no_grad():\n",
    "    model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "    model.eval()\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_predictions(probs, k=5):\n",
    "    probs = probs.detach().numpy()\n",
    "    top_indexes = np.argpartition(probs, -k)[-k:]\n",
    "    sorted_indexes = top_indexes[np.argsort(-probs[top_indexes])]\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(sorted_indexes)\n",
    "    print(f\"Ordered top predicted tokens: {top_tokens}\")\n",
    "    print(f\"Ordered top predicted values: {probs[sorted_indexes]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN = '[CLS]'\n",
    "EOS_TOKEN = '[SEP]'\n",
    "MASK_TOKEN = '[MASK]'\n",
    "\n",
    "def get_sentence_prob(sentence, verbose=False):\n",
    "    sm = torch.nn.Softmax(dim=0) # used to convert last hidden state to probs\n",
    "    \n",
    "    # Pre-process sentence, adding special tokens\n",
    "    tokenized_input = tokenizer.tokenize(sentence)\n",
    "    sent_len = len(tokenized_input)\n",
    "    if tokenized_input[0] != BOS_TOKEN:\n",
    "        tokenized_input.insert(0, BOS_TOKEN)\n",
    "    if tokenized_input[-1] != EOS_TOKEN:\n",
    "        tokenized_input.append(EOS_TOKEN)\n",
    "    ids_input = tokenizer.convert_tokens_to_ids(tokenized_input)\n",
    "    print(f\"Processing sentence: {tokenized_input}\")\n",
    "    #print(f\"Sentence ids: {ids_input}\")\n",
    "    \n",
    "    #sent_prob = 1\n",
    "    sum_lp = 0\n",
    "    # Mask non-special tokens and calculate their probabilities\n",
    "    for i in range(1,len(tokenized_input)-1): # Ignore first and last tokens\n",
    "        current_tokenized = tokenized_input[:]\n",
    "        current_tokenized[i] = MASK_TOKEN\n",
    "        if verbose: print(current_tokenized)\n",
    "        masked_input = torch.tensor([tokenizer.convert_tokens_to_ids(current_tokenized)])\n",
    "        outputs = model(masked_input)\n",
    "        predictions = outputs[0]\n",
    "        current_probs = sm(predictions[0, i]) # Softmax to get probabilities\n",
    "        current_prob = current_probs[ids_input[i]] # Prediction for masked word\n",
    "        #sent_prob *= current_prob\n",
    "        \n",
    "        sum_lp += np.log(current_prob.detach().numpy())\n",
    "        \n",
    "        print(f\"Word: {tokenized_input[i]} \\t Prob: {current_prob}\")\n",
    "        if verbose: print_top_predictions(current_probs)\n",
    "\n",
    "    #print(f\"\\nSentence probability: {sent_prob.item()}\\n\")\n",
    "    print(f\"\\nNormalized sentence prob: log(P(sentence)) / sent_length: {sum_lp / sent_len}\\n\")\n",
    "    return sum_lp / sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence: ['[CLS]', 'he', 'answered', 'une', '##qui', '##vo', '##cal', '##ly', '.', '[SEP]']\n",
      "Word: he \t Prob: 0.2814375162124634\n",
      "Word: answered \t Prob: 0.006721243727952242\n",
      "Word: une \t Prob: 0.9973625540733337\n",
      "Word: ##qui \t Prob: 0.9999865293502808\n",
      "Word: ##vo \t Prob: 0.9999856948852539\n",
      "Word: ##cal \t Prob: 0.9999865293502808\n",
      "Word: ##ly \t Prob: 0.9979932308197021\n",
      "Word: . \t Prob: 0.9998167157173157\n",
      "\n",
      "Normalized sentence prob: log(P(sentence)) / sent_length: -0.784400124636818\n",
      "\n",
      "Processing sentence: ['[CLS]', 'he', 'answered', 'quickly', '.', '[SEP]']\n",
      "Word: he \t Prob: 0.2151750773191452\n",
      "Word: answered \t Prob: 0.026344342157244682\n",
      "Word: quickly \t Prob: 0.05330450460314751\n",
      "Word: . \t Prob: 0.9981406927108765\n",
      "\n",
      "Normalized sentence prob: log(P(sentence)) / sent_length: -2.0266001676791348\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.0266001676791348"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_prob(\"He answered unequivocally.\")\n",
    "get_sentence_prob(\"He answered quickly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence: ['[CLS]', 'the', 'guy', 'with', 'small', 'hands', 'demanded', 'a', 'qui', '##d', 'pro', 'quo', '.', '[SEP]']\n",
      "Word: the \t Prob: 0.6742717027664185\n",
      "Word: guy \t Prob: 0.006106184795498848\n",
      "Word: with \t Prob: 0.9959086179733276\n",
      "Word: small \t Prob: 0.001629635225981474\n",
      "Word: hands \t Prob: 0.20016466081142426\n",
      "Word: demanded \t Prob: 0.03818148002028465\n",
      "Word: a \t Prob: 0.5014763474464417\n",
      "Word: qui \t Prob: 0.9985383749008179\n",
      "Word: ##d \t Prob: 0.9992328882217407\n",
      "Word: pro \t Prob: 0.9958876967430115\n",
      "Word: quo \t Prob: 0.9983682036399841\n",
      "Word: . \t Prob: 0.9850805401802063\n",
      "\n",
      "Normalized sentence prob: log(P(sentence)) / sent_length: -1.4586090460895018\n",
      "\n",
      "Processing sentence: ['[CLS]', 'the', 'guy', 'with', 'small', 'hands', 'demanded', 'an', 'exchange', '.', '[SEP]']\n",
      "Word: the \t Prob: 0.6960532069206238\n",
      "Word: guy \t Prob: 0.002731535118073225\n",
      "Word: with \t Prob: 0.9953562617301941\n",
      "Word: small \t Prob: 0.001821734826080501\n",
      "Word: hands \t Prob: 0.21409577131271362\n",
      "Word: demanded \t Prob: 0.21094851195812225\n",
      "Word: an \t Prob: 0.8602176904678345\n",
      "Word: exchange \t Prob: 0.0040652137249708176\n",
      "Word: . \t Prob: 0.9960935711860657\n",
      "\n",
      "Normalized sentence prob: log(P(sentence)) / sent_length: -2.3705652872514396\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.3705652872514396"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_prob(\"The guy with small hands demanded a quid pro quo.\")\n",
    "get_sentence_prob(\"The guy with small hands demanded an exchange.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence_prob(\"This is a sentence.\")\n",
    "get_sentence_prob(\"This is a macrame.\", verbose=False)\n",
    "get_sentence_prob(\"This is a joke.\", verbose=False)\n",
    "get_sentence_prob(\"Are you kidding.\", verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence_prob(\"Rachel was wearing a lovely satin dress last night.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence_prob(\"Rachel was wearing a lovely satin dress last night.\")\n",
    "get_sentence_prob(\"Grandma was wearing a lovely satin dress last night.\")\n",
    "get_sentence_prob(\"Mother was wearing a lovely satin dress last night.\")\n",
    "get_sentence_prob(\"She was wearing a lovely satin dress last night.\")\n",
    "get_sentence_prob(\"He was wearing a lovely satin dress last night.\")\n",
    "get_sentence_prob(\"I was wearing a lovely satin dress last night.\")\n",
    "get_sentence_prob(\"Angela was wearing a lovely satin dress last night.\")\n",
    "get_sentence_prob(\"Roberta was wearing a lovely satin dress last night.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence_prob(\"The man ate the steak.\")\n",
    "get_sentence_prob(\"The man who arrived late ate the steak with a glass of wine.\")\n",
    "get_sentence_prob(\"The steak was eaten by the man.\")\n",
    "get_sentence_prob(\"The stake ate the man.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_sentence_prob(\"I fed my cat some of it and he damn near passed out\")\n",
    "get_sentence_prob(\"He was born in Berlin.\")\n",
    "get_sentence_prob(\"He was born in Santiago.\")\n",
    "get_sentence_prob(\"He was born in France.\")\n",
    "get_sentence_prob(\"He was born in window.\")\n",
    "get_sentence_prob(\"He was born in was.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence_prob(\"I fed my cat some of it and he damn near passed out.\")\n",
    "get_sentence_prob(\"I fed my dog some of it and he damn near passed out.\")\n",
    "get_sentence_prob(\"I fed my window some of it and he damn near passed out.\")\n",
    "get_sentence_prob(\"I fed my the some of it and he damn near passed out.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Should have similar/high probs\\n\")\n",
    "get_sentence_prob(\"I forgot to take my medicine.\")\n",
    "get_sentence_prob(\"I forgot to take my medicines.\")\n",
    "get_sentence_prob(\"I forgot to take my medication.\")\n",
    "get_sentence_prob(\"I forgot to take my pills.\")\n",
    "print(\"Should have low probs\\n\")\n",
    "get_sentence_prob(\"I forgot to take my turn.\")\n",
    "get_sentence_prob(\"I forgot to take my medical.\")\n",
    "get_sentence_prob(\"I forgot to take my medically.\")\n",
    "get_sentence_prob(\"I forgot to take my turned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "with torch.no_grad():\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "    model.eval()\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_sentence_prob(\"I fed my cat some of it and he damn near passed out\")\n",
    "get_sentence_prob(\"He was born in Berlin.\")\n",
    "get_sentence_prob(\"He was born in Santiago.\")\n",
    "get_sentence_prob(\"He was born in France.\")\n",
    "get_sentence_prob(\"He was born in window.\")\n",
    "get_sentence_prob(\"He was born in was.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence_prob(\"I fed my cat some of it and he damn near passed out.\")\n",
    "get_sentence_prob(\"I fed my dog some of it and he damn near passed out.\")\n",
    "get_sentence_prob(\"I fed my window some of it and he damn near passed out.\")\n",
    "get_sentence_prob(\"I fed my the some of it and he damn near passed out.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Should have similar/high probs\\n\")\n",
    "get_sentence_prob(\"I forgot to take my medicine.\")\n",
    "get_sentence_prob(\"I forgot to take my medicines.\")\n",
    "get_sentence_prob(\"I forgot to take my medication.\")\n",
    "get_sentence_prob(\"I forgot to take my pills.\")\n",
    "print(\"Should have low probs\\n\")\n",
    "get_sentence_prob(\"I forgot to take my turn.\")\n",
    "get_sentence_prob(\"I forgot to take my medical.\")\n",
    "get_sentence_prob(\"I forgot to take my medically.\")\n",
    "get_sentence_prob(\"I forgot to take my turned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers]",
   "language": "python",
   "name": "conda-env-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
